{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "from math import ceil\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_actions, frame_count = 1, layer_channels = 16, input_shape=(96, 96, 3), lr = 1e-5, softmax = True):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.input_channels = input_shape[-1]\n",
    "        self.input_size = input_shape[:-1]\n",
    "        self.num_actions = num_actions\n",
    "        self.layer_channels = layer_channels\n",
    "        self.frame_count = frame_count\n",
    "\n",
    "        kernel_size = 5\n",
    "        conv_layers = [\n",
    "            nn.Conv2d(self.input_channels, layer_channels, bias = False, kernel_size = kernel_size, stride=2, padding = kernel_size//2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(layer_channels),\n",
    "            nn.Conv2d(layer_channels, layer_channels, kernel_size = kernel_size, bias = False, stride=1, padding = kernel_size//2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(layer_channels),\n",
    "            nn.Conv2d(layer_channels, layer_channels, kernel_size = kernel_size, bias = False, stride=2, padding = kernel_size//2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(layer_channels),\n",
    "            nn.Conv2d(layer_channels, layer_channels, kernel_size = kernel_size, bias = False, stride=1, padding = kernel_size//2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(layer_channels),\n",
    "            nn.Conv2d(layer_channels, layer_channels, kernel_size = kernel_size, bias = False, stride=2, padding = kernel_size//2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(layer_channels),\n",
    "            nn.Conv2d(layer_channels, layer_channels, kernel_size = kernel_size, bias = False, stride=1, padding = kernel_size//2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(layer_channels),\n",
    "            nn.Conv2d(layer_channels, layer_channels, kernel_size = kernel_size, bias = False, stride=2, padding = kernel_size//2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(layer_channels),\n",
    "            nn.Conv2d(layer_channels, layer_channels, kernel_size = kernel_size, bias = False, stride=1, padding = kernel_size//2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(layer_channels)]\n",
    "        \n",
    "        mlp_layers = [\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(layer_channels * ceil(self.input_size[0] / 2**4) * ceil(self.input_size[1] / 2**4), layer_channels),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(layer_channels, num_actions)\n",
    "        ]\n",
    "\n",
    "        self.softmax = softmax\n",
    "        if self.softmax:\n",
    "            mlp_layers.append(nn.Softmax(dim = -1))\n",
    "        self.q1 = nn.Sequential(*conv_layers)\n",
    "        self.q2 = nn.Sequential(*mlp_layers)\n",
    "\n",
    "        self.optim = Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2).float()\n",
    "        x = self.q1(x)\n",
    "        return self.q2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "model = CNN(3)\n",
    "print(model)\n",
    "summary(model, (96, 96, 3), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def train_qlearn_cuda(model, env, num_episodes=1000, gamma=0.9, epsilon_decay=0.9):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    # Create target model (on GPU)\n",
    "    target_model = deepcopy(model)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "    target_model.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Create CPU model for env interaction\n",
    "    cpu_model = deepcopy(model)\n",
    "    cpu_model.load_state_dict(model.state_dict())\n",
    "    cpu_model.to(\"cpu\")\n",
    "\n",
    "    current_index = 0\n",
    "    buffer_size = 4096\n",
    "    state_shape = env.observation_space.shape\n",
    "\n",
    "    # Keep replay buffer on GPU\n",
    "    states_buffer = torch.zeros((buffer_size, *state_shape), dtype=torch.float32, device=device)\n",
    "    actions_buffer = torch.zeros((buffer_size,), dtype=torch.int64, device=device)\n",
    "    rewards_buffer = torch.zeros((buffer_size,), dtype=torch.float32, device=device)\n",
    "    dones_buffer = torch.zeros((buffer_size,), dtype=torch.bool, device=device)\n",
    "    rewards_history = []\n",
    "\n",
    "    epsilon = 1\n",
    "    try:\n",
    "        for e in range(num_episodes):\n",
    "            state, *_ = env.reset()\n",
    "            state = torch.from_numpy(np.array(state)).float()  # keep on CPU for env\n",
    "            done = False\n",
    "\n",
    "            total_reward = 0\n",
    "\n",
    "            elapsed_time = 0\n",
    "            total_action_time = 0 \n",
    "            total_step_time = 0\n",
    "            total_sample_time = 0\n",
    "            total_q_time = 0\n",
    "            total_train_time = 0\n",
    "\n",
    "            while not done:\n",
    "                start_time = time.perf_counter()\n",
    "                # Choose action\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        states_tensor = state.unsqueeze(0)  # still on CPU\n",
    "                        q_values = cpu_model(states_tensor)\n",
    "                        action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "                action_time = time.perf_counter() \n",
    "                total_action_time += action_time - start_time\n",
    "\n",
    "                # Take action\n",
    "                next_state, reward, terminal, truncated, _ = env.step(action)\n",
    "                next_state = torch.from_numpy(np.array(next_state)).float()  # keep on CPU\n",
    "                done = terminal or truncated\n",
    "                total_reward += reward\n",
    "\n",
    "                # Store transition in memory (move to GPU)\n",
    "                index = current_index % buffer_size\n",
    "                states_buffer[index] = state.to(device)\n",
    "                actions_buffer[index] = torch.tensor(action, dtype=torch.int64, device=device)\n",
    "                rewards_buffer[index] = torch.tensor(reward, dtype=torch.float32, device=device)\n",
    "                dones_buffer[index] = torch.tensor(done, dtype=torch.bool, device=device)\n",
    "                current_index += 1\n",
    "\n",
    "                step_time = time.perf_counter()\n",
    "                total_step_time += step_time - action_time\n",
    "\n",
    "                # Update state\n",
    "                state = next_state\n",
    "\n",
    "            # Append total reward to history\n",
    "            rewards_history.append(total_reward)\n",
    "\n",
    "            # Train the model\n",
    "            if current_index > 1000:\n",
    "                batch_size = 128\n",
    "                for i in range(4 * current_index//batch_size):\n",
    "                    step_time = time.perf_counter()\n",
    "                    idx = np.random.choice(min(current_index, buffer_size) - 1, batch_size, replace=False)\n",
    "\n",
    "                    # All buffers are already on GPU\n",
    "                    states_tensor = states_buffer[idx]\n",
    "                    actions_tensor = actions_buffer[idx]\n",
    "                    rewards_tensor = rewards_buffer[idx]\n",
    "                    next_states_tensor = states_buffer[idx+1]\n",
    "                    dones_tensor = dones_buffer[idx]\n",
    "\n",
    "                    sample_time = time.perf_counter()\n",
    "                    total_sample_time += sample_time - step_time\n",
    "\n",
    "                    # Double Q-Learning\n",
    "                    q_values = model(states_tensor).gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
    "                    # Action selection using online network\n",
    "                    next_actions = model(next_states_tensor).argmax(1)\n",
    "                    # Action evaluation using target network\n",
    "                    next_q_values = target_model(next_states_tensor).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "                    expected_q_values = rewards_tensor + (gamma * next_q_values * (~dones_tensor))\n",
    "\n",
    "                    q_time = time.perf_counter()\n",
    "                    total_q_time += q_time - sample_time\n",
    "\n",
    "                    loss = nn.MSELoss()(q_values.float(), expected_q_values.float())\n",
    "\n",
    "                    model.optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    model.optim.step()\n",
    "\n",
    "                    # After each update, copy weights to cpu_model\n",
    "                    train_time = time.perf_counter()\n",
    "                    total_train_time += train_time - q_time\n",
    "\n",
    "                cpu_model.load_state_dict(model.state_dict())\n",
    "                \n",
    "            # Update target model\n",
    "            if e % 10 == 0:\n",
    "                target_model.load_state_dict(model.state_dict())\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            elapsed_time += end_time - start_time\n",
    "\n",
    "            epsilon *= epsilon_decay\n",
    "            if epsilon < 0.01:\n",
    "                epsilon = 0.01\n",
    "            print(f\"Episode {e+1}/{num_episodes}, Total Reward: {total_reward}, Time Elapsed: {elapsed_time} seconds\")\n",
    "            print(f\"Action Time: {total_action_time:.4f}, Step Time: {total_step_time:.4f}, Sample Time: {total_sample_time:.4f}, Q Time: {total_q_time:.4f}, Train Time: {total_train_time:.4f}\")\n",
    "\n",
    "        print(\"Training completed successfully.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user.\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return rewards_history\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return rewards_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "def train_qlearn(model, env, num_episodes=1000, gamma=0.9, epsilon_decay=0.9):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    # Create target model (on GPU)\n",
    "    target_model = deepcopy(model)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "    target_model.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Create CPU model for env interaction\n",
    "    cpu_model = deepcopy(model)\n",
    "    cpu_model.load_state_dict(model.state_dict())\n",
    "    cpu_model.to(\"cpu\")\n",
    "\n",
    "    current_index = 0\n",
    "    buffer_size = 4096\n",
    "    batch_size = 256\n",
    "    state_shape = env.observation_space.shape\n",
    "\n",
    "    states_buffer = np.empty((buffer_size, *state_shape))\n",
    "    actions_buffer = np.empty((buffer_size), dtype = np.int64)\n",
    "    rewards_buffer = np.empty((buffer_size))\n",
    "    next_states_buffer = np.empty((buffer_size, *state_shape))\n",
    "    dones_buffer = np.empty((buffer_size))\n",
    "\n",
    "    rewards_history = []\n",
    "\n",
    "    epsilon = 1\n",
    "    try:\n",
    "        for e in range(num_episodes):\n",
    "            state, *_ = env.reset()\n",
    "            done = False\n",
    "\n",
    "            total_reward = 0\n",
    "\n",
    "            elapsed_time = 0\n",
    "            total_action_time = 0 \n",
    "            total_step_time = 0\n",
    "            total_sample_time = 0\n",
    "            total_q_time = 0\n",
    "            total_train_time = 0\n",
    "\n",
    "            while not done:\n",
    "                start_time = time.perf_counter()\n",
    "                # Choose action\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        states_tensor = torch.from_numpy(np.array(state)).float().unsqueeze(0)  # keep on CPU\n",
    "                        q_values = cpu_model(states_tensor)\n",
    "                        action = np.argmax(q_values)\n",
    "\n",
    "                action_time = time.perf_counter() \n",
    "                total_action_time += action_time - start_time\n",
    "\n",
    "                # Take action\n",
    "                next_state, reward, terminal, truncated, _ = env.step(action)\n",
    "                done = terminal or truncated\n",
    "                total_reward += reward\n",
    "\n",
    "                # Store transition in memory (move to GPU)\n",
    "                idx = current_index % buffer_size\n",
    "                states_buffer[idx] = state\n",
    "                actions_buffer[idx] = action\n",
    "                rewards_buffer[idx] = reward\n",
    "                next_states_buffer[idx] = next_state\n",
    "                dones_buffer[idx] = done\n",
    "                current_index += 1\n",
    "\n",
    "                step_time = time.perf_counter()\n",
    "                total_step_time += step_time - action_time\n",
    "\n",
    "                # Update state\n",
    "                state = next_state\n",
    "\n",
    "            # Append total reward to history\n",
    "            rewards_history.append(total_reward)\n",
    "\n",
    "            # Train the model\n",
    "            if current_index > 1000:\n",
    "                max_idx = min(current_index, buffer_size)\n",
    "                for i in range(max_idx//batch_size):\n",
    "                    step_time = time.perf_counter()\n",
    "                    \n",
    "                    idx = np.random.choice(max_idx, batch_size, replace=False)\n",
    "                    # All buffers are already on GPU\n",
    "                    states_tensor = torch.from_numpy(states_buffer[idx]).pin_memory().to(device, non_blocking = True)\n",
    "                    actions_tensor = torch.from_numpy(actions_buffer[idx]).pin_memory().to(device, non_blocking = True)\n",
    "                    rewards_tensor = torch.from_numpy(rewards_buffer[idx]).pin_memory().to(device, non_blocking = True)\n",
    "                    next_states_tensor = torch.from_numpy(next_states_buffer[idx]).pin_memory().to(device, non_blocking = True)\n",
    "                    dones_tensor = torch.from_numpy(dones_buffer[idx]).pin_memory().to(device, non_blocking = True)\n",
    "\n",
    "                    sample_time = time.perf_counter()\n",
    "                    total_sample_time += sample_time - step_time\n",
    "\n",
    "                    # Double Q-Learning\n",
    "                    q_values = model(states_tensor).gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
    "                    # Action selection using online network\n",
    "                    next_actions = model(next_states_tensor).argmax(1)\n",
    "                    # Action evaluation using target network\n",
    "                    next_q_values = target_model(next_states_tensor).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "                    expected_q_values = rewards_tensor + (gamma * next_q_values * (dones_tensor))\n",
    "\n",
    "                    q_time = time.perf_counter()\n",
    "                    total_q_time += q_time - sample_time\n",
    "\n",
    "                    loss = nn.MSELoss()(q_values.float(), expected_q_values.float())\n",
    "\n",
    "                    model.optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    model.optim.step()\n",
    "\n",
    "                    # After each update, copy weights to cpu_model\n",
    "                    train_time = time.perf_counter()\n",
    "                    total_train_time += train_time - q_time\n",
    "\n",
    "                cpu_model.load_state_dict(model.state_dict())\n",
    "                \n",
    "            # Update target model\n",
    "            if e % 10 == 0:\n",
    "                target_model.load_state_dict(model.state_dict())\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            elapsed_time += end_time - start_time\n",
    "\n",
    "            epsilon *= epsilon_decay\n",
    "            if epsilon < 0.01:\n",
    "                epsilon = 0.01\n",
    "            print(f\"Episode {e+1}/{num_episodes}, Total Reward: {total_reward}, Time Elapsed: {elapsed_time} seconds\")\n",
    "            print(f\"Action Time: {total_action_time:.4f}, Step Time: {total_step_time:.4f}, Sample Time: {total_sample_time:.4f}, Q Time: {total_q_time:.4f}, Train Time: {total_train_time:.4f}\")\n",
    "\n",
    "        print(\"Training completed successfully.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user.\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return rewards_history\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return rewards_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "env = gym.make(\"ALE/Pacman-v5\")\n",
    "#env = gym.make(\"CarRacing-v3\")\n",
    "\n",
    "#model = CNN(env.action_space.shape)\n",
    "device = \"cuda\"\n",
    "print(env.observation_space, env.action_space)\n",
    "state_shape = env.observation_space.shape\n",
    "action_shape = 5 #env.action_space.shape\n",
    "model = CNN(action_shape, input_shape = state_shape).to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "rewards = train_qlearn(model, env, num_episodes=1000, gamma=0.9)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qmodel(model):\n",
    "    env = gym.make(\"CarRacing-v3\", continuous=False, render_mode=\"human\")\n",
    "    state, *_ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "        action = q_values.to(\"cpu\").argmax().item()\n",
    "\n",
    "        next_state, reward, terminal, truncated, info = env.step(action)\n",
    "        done = terminal or truncated\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        time.sleep(0.01) # Adjust sleep time for rendering speed\n",
    "\n",
    "    env.close()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "def train_reinforce(model, env, num_episodes=1000, gamma=0.9):\n",
    "    ## Create target model\n",
    "    target_model = deepcopy(model)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    target_model.to(device)\n",
    "\n",
    "    rewards_history = []\n",
    "\n",
    "    all_actions = list(range(env.action_space.n))\n",
    "    try:\n",
    "        for e in range(num_episodes):\n",
    "            #buffer_size = 1024\n",
    "            #replay_buffer = deque(maxlen=buffer_size)\n",
    "            replay_buffer = []\n",
    "\n",
    "            state, *_ = env.reset()\n",
    "            done = False\n",
    "\n",
    "            total_reward = 0\n",
    "\n",
    "            elapsed_time = 0\n",
    "            total_action_time = 0 \n",
    "            total_step_time = 0\n",
    "            total_sample_time = 0\n",
    "            total_q_time = 0\n",
    "            total_train_time = 0\n",
    "\n",
    "            #for t in range(buffer_size):\n",
    "            while True:\n",
    "                start_time = time.perf_counter()\n",
    "                # Choose action\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "                    action_dist = model(state_tensor)\n",
    "                    action_dist = action_dist.squeeze(0).cpu().numpy()\n",
    "                    action = np.random.choice(all_actions, p=action_dist)\n",
    "                \n",
    "                action_time = time.perf_counter() \n",
    "                total_action_time += action_time - start_time\n",
    "\n",
    "                # Take action\n",
    "                next_state, reward, terminal, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                replay_buffer.append([state, action, reward])\n",
    "\n",
    "                step_time = time.perf_counter()\n",
    "                total_step_time += step_time - action_time\n",
    "\n",
    "                # Update state\n",
    "                state = next_state\n",
    "                done = terminal or truncated\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            running_rewards = 0\n",
    "            for i in range(len(replay_buffer)):\n",
    "                r = replay_buffer[-i-1][2]\n",
    "                running_rewards = r + gamma * running_rewards\n",
    "                replay_buffer[-i-1][2] = running_rewards\n",
    "\n",
    "            sample_time = time.perf_counter()\n",
    "            total_sample_time += sample_time - step_time\n",
    "            \n",
    "            states_tensor = torch.tensor(np.array([t[0] for t in replay_buffer]), dtype=torch.float32, device=device)\n",
    "            preds = model(states_tensor)\n",
    "            \n",
    "            q_time = time.perf_counter()\n",
    "            total_q_time += q_time - sample_time\n",
    "\n",
    "            actions = torch.tensor(np.array([t[1] for t in replay_buffer]), dtype=torch.long, device=device)\n",
    "            discounted_rewards = torch.tensor(np.array([t[2] for t in replay_buffer]), dtype=torch.float32, device=device)\n",
    "            log_probs = torch.log(preds.gather(1, actions.unsqueeze(1)).squeeze(1) + 1e-8)\n",
    "            loss = -(log_probs * discounted_rewards).sum()\n",
    "\n",
    "            model.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "\n",
    "            train_time = time.perf_counter()\n",
    "            total_train_time += train_time - q_time\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            elapsed_time += end_time - start_time\n",
    "\n",
    "            # Append total reward to history\n",
    "            rewards_history.append(total_reward)\n",
    "\n",
    "            print(f\"Episode {e+1}/{num_episodes}, Total Reward: {total_reward}, Time Elapsed: {elapsed_time} seconds\")\n",
    "            print(f\"Action Time: {total_action_time:.4f}, Step Time: {total_step_time:.4f}, Sample Time: {total_sample_time:.4f}, Q Time: {total_q_time:.4f}, Train Time: {total_train_time:.4f}\")\n",
    "        \n",
    "        print(\"Training completed successfully.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user.\")\n",
    "        return rewards_history\n",
    "    \n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "def train_reinforce_frames(model, env, nframes = 1, num_episodes=1000, gamma=0.9):\n",
    "    ## Create target model\n",
    "    target_model = deepcopy(model)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    target_model.to(device)\n",
    "\n",
    "    rewards_history = []\n",
    "\n",
    "    all_actions = list(range(env.action_space.n))\n",
    "    try:\n",
    "        for e in range(num_episodes):\n",
    "            #buffer_size = 1024\n",
    "            #replay_buffer = deque(maxlen=buffer_size)\n",
    "            replay_buffer = []\n",
    "\n",
    "            state, *_ = env.reset()\n",
    "            done = False\n",
    "\n",
    "            # Initialize deque to hold previous states\n",
    "            previous_states = deque(maxlen=nframes)\n",
    "            previous_states.append(state)\n",
    "\n",
    "            for _ in range(nframes - 1):\n",
    "                # skip first frames\n",
    "                state, *_ = env.step(0)\n",
    "                previous_states.append(state)\n",
    "\n",
    "            total_reward = 0\n",
    "\n",
    "            elapsed_time = 0\n",
    "            total_action_time = 0 \n",
    "            total_step_time = 0\n",
    "            total_sample_time = 0\n",
    "            total_q_time = 0\n",
    "            total_train_time = 0\n",
    "\n",
    "            #for t in range(buffer_size):\n",
    "            while True:\n",
    "                start_time = time.perf_counter()\n",
    "                # Choose action\n",
    "                with torch.no_grad():\n",
    "                    prev_states = np.concatenate(previous_states, axis = -1)\n",
    "                    state_tensor = torch.from_numpy(prev_states).unsqueeze(0).to(device)\n",
    "                    action_dist = model(state_tensor)\n",
    "                    action_dist = action_dist.squeeze(0).cpu().numpy()\n",
    "                    action = np.random.choice(all_actions, p=action_dist)\n",
    "                \n",
    "                action_time = time.perf_counter() \n",
    "                total_action_time += action_time - start_time\n",
    "\n",
    "                # Take action\n",
    "                next_state, reward, terminal, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                replay_buffer.append([previous_states, action, reward])\n",
    "\n",
    "                step_time = time.perf_counter()\n",
    "                total_step_time += step_time - action_time\n",
    "\n",
    "                # Update state\n",
    "                #state = next_state\n",
    "                previous_states.append(next_state)\n",
    "\n",
    "                done = terminal or truncated\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            running_rewards = 0\n",
    "            for i in range(len(replay_buffer)):\n",
    "                r = replay_buffer[-i-1][2]\n",
    "                running_rewards = r + gamma * running_rewards\n",
    "                replay_buffer[-i-1][2] = running_rewards\n",
    "\n",
    "            sample_time = time.perf_counter()\n",
    "            total_sample_time += sample_time - step_time\n",
    "            \n",
    "            states_tensor = torch.tensor(np.array([t[0] for t in replay_buffer]), dtype=torch.float32, device=device)\n",
    "            preds = model(states_tensor)\n",
    "            \n",
    "            q_time = time.perf_counter()\n",
    "            total_q_time += q_time - sample_time\n",
    "\n",
    "            actions = torch.tensor(np.array([t[1] for t in replay_buffer]), dtype=torch.long, device=device)\n",
    "            discounted_rewards = torch.tensor(np.array([t[2] for t in replay_buffer]), dtype=torch.float32, device=device)\n",
    "            log_probs = torch.log(preds.gather(1, actions.unsqueeze(1)).squeeze(1) + 1e-8)\n",
    "            loss = -(log_probs * discounted_rewards).sum()\n",
    "\n",
    "            model.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "\n",
    "            train_time = time.perf_counter()\n",
    "            total_train_time += train_time - q_time\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            elapsed_time += end_time - start_time\n",
    "\n",
    "            # Append total reward to history\n",
    "            rewards_history.append(total_reward)\n",
    "\n",
    "            print(f\"Episode {e+1}/{num_episodes}, Total Reward: {total_reward}, Time Elapsed: {elapsed_time} seconds\")\n",
    "            print(f\"Action Time: {total_action_time:.4f}, Step Time: {total_step_time:.4f}, Sample Time: {total_sample_time:.4f}, Q Time: {total_q_time:.4f}, Train Time: {total_train_time:.4f}\")\n",
    "        \n",
    "        print(\"Training completed successfully.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user.\")\n",
    "        return rewards_history\n",
    "    \n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py\n",
    "#env = gym.make(\"ALE/Riverraid-v5\")\n",
    "env = gym.make(\"ALE/Pacman-v5\")\n",
    "#env = gym.make(\"CarRacing-v3\", continuous = False)\n",
    "\n",
    "#model = CNN(env.action_space.shape)\n",
    "device = \"cpu\"\n",
    "print(env.observation_space, env.action_space)\n",
    "state_shape = env.observation_space.shape\n",
    "action_shape = 5 #env.action_space.shape\n",
    "model = CNN(action_shape, input_shape = state_shape).to(device)\n",
    "\n",
    "rewards = train_reinforce(model, env, num_episodes=1000, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", continuous = False)\n",
    "\n",
    "#model = CNN(env.action_space.shape)\n",
    "device = \"cpu\"\n",
    "model = CNN(5).to(device)\n",
    "\n",
    "rewards = train_reinforce(model, env, num_episodes=1000, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
